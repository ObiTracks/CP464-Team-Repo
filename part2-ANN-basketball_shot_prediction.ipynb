{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "\n",
    "# Note: Make sure to replace '/path/to/save/model' and '/path/to/norm_params.csv' with actual paths where you want to save your model and normalization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dataset files:  4449\n",
      "Total number dataset rows:  4613003\n"
     ]
    }
   ],
   "source": [
    "# Load and combine datasets\n",
    "dataset_path = \"datasets/nba-shot-chart-dataset-2000-2024\"\n",
    "file_paths = [os.path.join(dataset_path, file_name) for file_name in os.listdir(dataset_path) if file_name.endswith('.csv')]\n",
    "print(\"Number of dataset files: \", len(file_paths))\n",
    "\n",
    "# Load and combine datasets dynamically\n",
    "dfs = [pd.read_csv(file_path) for file_path in file_paths]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "print(\"Total number dataset rows: \", combined_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and preprocessing\n",
    "cleaned_df = combined_df.dropna(subset=['made', 'shotX', 'shotY'])\n",
    "cleaned_df['shotX'] = (cleaned_df['shotX'] - cleaned_df['shotX'].mean()) / cleaned_df['shotX'].std()\n",
    "cleaned_df['shotY'] = (cleaned_df['shotY'] - cleaned_df['shotY'].mean()) / cleaned_df['shotY'].std()\n",
    "cleaned_df['made'] = cleaned_df['made'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global normalization\n",
    "x_mean = cleaned_df['shotX'].mean()\n",
    "x_std = cleaned_df['shotX'].std()\n",
    "y_mean = cleaned_df['shotY'].mean()\n",
    "y_std = cleaned_df['shotY'].std()\n",
    "\n",
    "cleaned_df['shotX'] = (cleaned_df['shotX'] - x_mean) / x_std\n",
    "cleaned_df['shotY'] = (cleaned_df['shotY'] - y_mean) / y_std\n",
    "cleaned_df['made'] = cleaned_df['made'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save normalization parameters for use in prediction script\n",
    "norm_params = {'x_mean': x_mean, 'x_std': x_std, 'y_mean': y_mean, 'y_std': y_std}\n",
    "norm_params_df = pd.DataFrame(norm_params, index=[0])\n",
    "norm_params_df.to_csv('norm_params.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for model\n",
    "X = cleaned_df[['shotX', 'shotY']]\n",
    "y = cleaned_df['made']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "# print(X_train)\n",
    "\n",
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Model compilation\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m92261/92261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 665us/step - accuracy: 0.6149 - loss: 0.6650 - val_accuracy: 0.6172 - val_loss: 0.6622\n",
      "Epoch 2/10\n",
      "\u001b[1m92261/92261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 596us/step - accuracy: 0.6167 - loss: 0.6632 - val_accuracy: 0.6178 - val_loss: 0.6621\n",
      "Epoch 3/10\n",
      "\u001b[1m92261/92261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 608us/step - accuracy: 0.6173 - loss: 0.6627 - val_accuracy: 0.6178 - val_loss: 0.6620\n",
      "Epoch 4/10\n",
      "\u001b[1m92261/92261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 570us/step - accuracy: 0.6169 - loss: 0.6630 - val_accuracy: 0.6158 - val_loss: 0.6627\n",
      "Epoch 5/10\n",
      "\u001b[1m92261/92261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 586us/step - accuracy: 0.6172 - loss: 0.6629 - val_accuracy: 0.6169 - val_loss: 0.6622\n",
      "Epoch 6/10\n",
      "\u001b[1m92261/92261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 594us/step - accuracy: 0.6174 - loss: 0.6629 - val_accuracy: 0.6174 - val_loss: 0.6635\n",
      "Epoch 7/10\n",
      "\u001b[1m92261/92261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 709us/step - accuracy: 0.6167 - loss: 0.6633 - val_accuracy: 0.6169 - val_loss: 0.6623\n",
      "Epoch 8/10\n",
      "\u001b[1m92261/92261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 570us/step - accuracy: 0.6175 - loss: 0.6628 - val_accuracy: 0.6166 - val_loss: 0.6629\n",
      "Epoch 9/10\n",
      "\u001b[1m92261/92261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 655us/step - accuracy: 0.6169 - loss: 0.6631 - val_accuracy: 0.6169 - val_loss: 0.6626\n",
      "Epoch 10/10\n",
      "\u001b[1m92261/92261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 602us/step - accuracy: 0.6169 - loss: 0.6633 - val_accuracy: 0.6179 - val_loss: 0.6626\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28832/28832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 237us/step - accuracy: 0.6191 - loss: 0.6606\n",
      "Test Loss: 0.660916805267334, Test Accuracy: 0.6183420419692993\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "evaluation = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def save_model_with_version_control(model, base_directory='models', base_name='nba_shot_chart_model'):\n",
    "    # Create the models directory if it doesn't exist\n",
    "    os.makedirs(base_directory, exist_ok=True)\n",
    "    \n",
    "    # Define the base model path and pattern to search for existing versions\n",
    "    base_model_path = os.path.join(base_directory, f\"{base_name}.keras\")\n",
    "    version_pattern = re.compile(rf\"{base_name}_v(\\d+)\\.keras$\")\n",
    "    \n",
    "    # Find the highest version number\n",
    "    highest_version = 0\n",
    "    for filename in os.listdir(base_directory):\n",
    "        match = version_pattern.match(filename)\n",
    "        if match:\n",
    "            highest_version = max(highest_version, int(match.group(1)))\n",
    "    \n",
    "    # If the current model exists, rename it to the next version number\n",
    "    if os.path.exists(base_model_path):\n",
    "        new_version_path = os.path.join(base_directory, f\"{base_name}_v{highest_version + 1}.keras\")\n",
    "        os.rename(base_model_path, new_version_path)\n",
    "    \n",
    "    # Save the new current model\n",
    "    model.save(base_model_path)\n",
    "\n",
    "# Usage\n",
    "save_model_with_version_control(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
